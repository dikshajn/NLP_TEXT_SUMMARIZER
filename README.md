## NLP based text Summarization by BERT

As we proposed, transformer-based summarization may be achieved by using relevant user comments. Our methodology replicates the natural link between relevant user posts and the content of the primary documents by providing information in the form of key phrases or tokens.

To do this, we supply the model with two critical components: the use of social information and the power of transformers, i.e. BERT. More exactly, relevant user postings are used to supplement the information included in phrases in the primary papers. The enrichment is a mix of hidden characteristics of input texts and user postings discovered by BERT.

To capture more fine-grained hidden representations, we layer an extra convolution neural network (CNN) on top of BERT for classification. The final summary is generated by picking the top m ranked phrases based on their relevance, expressed as probability.


